{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install all the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install xlrd\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install numpy\n",
    "%pip install scikit-learn\n",
    "%pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import The Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Data Loading and Initial Exploration\n",
    "\n",
    "Lecture material: Lecture 3, slides 4–8, 10, and 11.\n",
    "\n",
    "- Load the dataset into a Pandas DataFrame.\n",
    "- Perform basic exploratory data analysis (EDA) to comprehend the structure and characteristics of the data.\n",
    "\n",
    "Note: Your analysis should include appropriate exploratory statistics and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Titanic dataset\n",
    "df = pd.read_excel('titanic3.xls')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Get summary info of the dataset \n",
    "print(df.info())\n",
    "\n",
    "# Get descriptive statistics for numerical valeus\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Managing Missing Values\n",
    "\n",
    "Lecture Material: Lecture 3, slides 22–24.\n",
    "\n",
    "- Identify the columns containing missing values.\n",
    "- Develop a strategy to address them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in each column\n",
    "missing_data = df.isnull().sum()\n",
    "\n",
    "# Display the columns that have missing values and how many\n",
    "missing_data = missing_data[missing_data > 0]\n",
    "print(missing_data)\n",
    "\n",
    "## Filling missing values\n",
    "## I decided to simply fill the missing values with the mode and means rather than erase them\n",
    "# Imputing numerical columns with median values\n",
    "df['age'] = df['age'].fillna(df['age'].median())\n",
    "df['fare'] = df['fare'].fillna(df['fare'].median())\n",
    "df['body'] = df['body'].fillna(df['body'].median())\n",
    "\n",
    "# Imputing categorical columns with the mode value\n",
    "df['embarked'] = df['embarked'].fillna(df['embarked'].mode()[0])\n",
    "df['cabin'] = df['cabin'].fillna(df['cabin'].mode()[0])\n",
    "df['boat'] = df['boat'].fillna(df['boat'].mode()[0])\n",
    "df['home.dest'] = df['home.dest'].fillna(df['home.dest'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Encoding Categorical Variables\n",
    "\n",
    "Lecture material: Lecture 4, slides 10–15, 21.\n",
    "\n",
    "- Identify the categorical variables in the dataset.\n",
    "- Utilize OneHotEncoder to encode them.\n",
    "- Observe the transformation and discuss its impact on machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the categorical columns for encoding\n",
    "categorical_columns = ['sex',  'embarked', 'survived', 'pclass']\n",
    "\n",
    "# Create a ColumnTransformer that applies OneHotEncoder to the categorical columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), categorical_columns)\n",
    "    ],\n",
    "    remainder='passthrough'  \n",
    ")\n",
    "\n",
    "# Apply the encoding to the dataset using a pipeline and convert them to a DataFrame\n",
    "df_encoded = preprocessor.fit_transform(df)\n",
    "encoded_columns = preprocessor.transformers_[0][1].get_feature_names_out(categorical_columns)\n",
    "df_encoded = pd.DataFrame(df_encoded, columns=np.append(encoded_columns, df.columns[len(categorical_columns):]))\n",
    "\n",
    "# Check the result\n",
    "print(df_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Feature Scaling\n",
    "\n",
    "Lecture material: Lecture 5, slides 14–20.\n",
    "\n",
    "- Standardize the numerical variables using StandardScaler.\n",
    "- Normalize the numerical variables using MinMaxScaler.\n",
    "- Discuss the differences between standardization and normalization, along with their importance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the numerical columns to standardize and Fit and transform the numerical columns to standardize them\n",
    "numerical_columns = ['age', 'fare', 'sibsp', 'parch', 'body']\n",
    "scaler = StandardScaler()\n",
    "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "# Check the result\n",
    "print(df[numerical_columns].head())\n",
    "\n",
    "# Create the MinMaxScaler instance and Fit and transform the numerical columns to normalize them\n",
    "min_max_scaler = MinMaxScaler()\n",
    "df[numerical_columns] = min_max_scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "# Check the result\n",
    "print(df[numerical_columns].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Data Splitting\n",
    "\n",
    "Lecture material: Lecture 2, slides 4–7.\n",
    "\n",
    "- Split the dataset into training, validation, and test sets.\n",
    "- Ensure that the split reflects the original distribution of the target variable using stratification.\n",
    "\n",
    "**Note**: a good strategy is to first split the dataset into ‘training’ and ‘others’, and then split ‘others’ into equally sized ‘validation’ and ‘test’ sets. When splitting sets, consider the argument stratify of the train test split\n",
    "method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['survived'])  # Features (excluding target variable)\n",
    "y = df['survived']  # Target variable (survival)\n",
    "\n",
    "# Split into training and 'others' (validation and test)\n",
    "X_train, X_others, y_train, y_others = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n",
    "\n",
    "# Split 'others' into validation and test sets, maintaining target distribution\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_others, y_others, test_size=0.5, stratify=y_others, random_state=42)\n",
    "\n",
    "# Check the distribution of the target variable in each split\n",
    "print(\"Training set target distribution:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nValidation set target distribution:\")\n",
    "print(y_val.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nTest set target distribution:\")\n",
    "print(y_test.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Addressing Class Imbalance\n",
    "\n",
    "Lecture material: Lecture 3, slides 25–27; Lecture 4, slides 4–5.\n",
    "\n",
    "- Apply a method to address class imbalance (e.g., Oversampling Technique (SMOTE), Adaptive Synthetic\n",
    "Sampling Method (ADASYN)).\n",
    "\n",
    "**Note**: You can load a SMOTE and/or ADASYN implementation from the Python module imblearn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare the dataset\n",
    "# Encode categorical variables (if any) and handle missing values\n",
    "df_encoded = pd.get_dummies(df, drop_first=True)\n",
    "X = df_encoded.drop(columns=['pclass'])\n",
    "y = df_encoded['pclass']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling the features \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Check the class distribution before applying SMOTE\n",
    "print(\"Original class distribution in the training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "# Impute missing values using the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train_scaled)\n",
    "X_test_imputed = imputer.transform(X_test_scaled)\n",
    "\n",
    "## Apply SMOTE to balance the classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_imputed, y_train)\n",
    "\n",
    "# Check class distribution after SMOTE\n",
    "print(\"\\nClass distribution after applying SMOTE:\")\n",
    "print(pd.Series(y_train_resampled).value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Feature Selection\n",
    "\n",
    "Lecture material: Lecture 5, slides 10–14, 19.\n",
    "\n",
    "- Eliminate low variance and highly correlated features.\n",
    "- Why do we carry out tasks 6 and 7 after splitting the dataset into training, validation, and test sets? Could\n",
    "we have conducted them on the entire dataset instead? Please elaborate on your answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "## Drop non-numeric columns from X_train \n",
    "X_train_numeric = X_train.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Step 2: Remove Low Variance Features\n",
    "variance_threshold = VarianceThreshold(threshold=0.01)  # (Set a threshold to remove low variance features)\n",
    "X_train_no_low_variance = variance_threshold.fit_transform(X_train_numeric)\n",
    "\n",
    "# Step 3: Get the feature names of the remaining features after variance reduction\n",
    "remaining_features = X_train_numeric.columns[variance_threshold.get_support()]\n",
    "print(f\"Remaining features after variance thresholding: {remaining_features}\")\n",
    "\n",
    "# Create a new DataFrame with only the selected features\n",
    "X_train_no_low_variance_df = pd.DataFrame(X_train_no_low_variance, columns=remaining_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: Training a Logistic Regression Model\n",
    "\n",
    "Lecture material: Lecture 6, slides 5–9.\n",
    "\n",
    "- Train a Logistic Regression Model to predict whether a passenger survives.\n",
    "\n",
    "**Note**: Use the method predict from the class LogisticRegression with the validation set. Have fun finding\n",
    "a visually appealing way to display the results of the predictions on the validation set. An analysis of model\n",
    "performance is not required and will not affect your final grade for the assignment. However, I won’t stop you from\n",
    "including it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Titanic3.xls dataset\n",
    "df = pd.read_excel('titanic3.xls')\n",
    "\n",
    "# Handling missing values \n",
    "df['age'] = df['age'].fillna(df['age'].median())\n",
    "df['fare'] = df['fare'].fillna(df['fare'].median())\n",
    "df['embarked'] = df['embarked'].fillna(df['embarked'].mode()[0])\n",
    "df['cabin'] = df['cabin'].fillna(df['cabin'].mode()[0])\n",
    "df['boat'] = df['boat'].fillna(df['boat'].mode()[0])\n",
    "df = df.dropna(subset=['survived', 'sex'])\n",
    "\n",
    "# Encoding categorical features using one-hot encoding\n",
    "df = pd.get_dummies(df, columns=['sex', 'embarked', 'cabin'], drop_first=True)\n",
    "\n",
    "# Selecting relevant features (excluding 'name', 'ticket' that dont affect the survival)\n",
    "X = df[['age', 'fare', 'sibsp', 'parch', 'sex_male', 'embarked_Q', 'embarked_S']]\n",
    "y = df['survived']\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Training the Logistic Regression model\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = logreg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Displaying results\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
