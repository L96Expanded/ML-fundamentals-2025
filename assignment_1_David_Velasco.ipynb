{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install all the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\megat\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\megat\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlrd in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\megat\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (2.2.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\megat\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\megat\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from seaborn) (3.10.0)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\megat\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\megat\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\megat\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (2.2.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imblearn) (0.13.0)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn->imblearn) (1.15.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn->imblearn) (1.6.1)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn->imblearn) (2.2.3)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in c:\\users\\megat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn->imblearn) (0.1.3)\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install xlrd\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install numpy\n",
    "%pip install scikit-learn\n",
    "%pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import The Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Data Loading and Initial Exploration\n",
    "\n",
    "Lecture material: Lecture 3, slides 4–8, 10, and 11.\n",
    "\n",
    "- Load the dataset into a Pandas DataFrame.\n",
    "- Perform basic exploratory data analysis (EDA) to comprehend the structure and characteristics of the data.\n",
    "\n",
    "Note: Your analysis should include appropriate exploratory statistics and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pclass  survived                                             name     sex  \\\n",
      "0       1         1                    Allen, Miss. Elisabeth Walton  female   \n",
      "1       1         1                   Allison, Master. Hudson Trevor    male   \n",
      "2       1         0                     Allison, Miss. Helen Loraine  female   \n",
      "3       1         0             Allison, Mr. Hudson Joshua Creighton    male   \n",
      "4       1         0  Allison, Mrs. Hudson J C (Bessie Waldo Daniels)  female   \n",
      "\n",
      "       age  sibsp  parch  ticket      fare    cabin embarked boat   body  \\\n",
      "0  29.0000      0      0   24160  211.3375       B5        S    2    NaN   \n",
      "1   0.9167      1      2  113781  151.5500  C22 C26        S   11    NaN   \n",
      "2   2.0000      1      2  113781  151.5500  C22 C26        S  NaN    NaN   \n",
      "3  30.0000      1      2  113781  151.5500  C22 C26        S  NaN  135.0   \n",
      "4  25.0000      1      2  113781  151.5500  C22 C26        S  NaN    NaN   \n",
      "\n",
      "                         home.dest  \n",
      "0                     St Louis, MO  \n",
      "1  Montreal, PQ / Chesterville, ON  \n",
      "2  Montreal, PQ / Chesterville, ON  \n",
      "3  Montreal, PQ / Chesterville, ON  \n",
      "4  Montreal, PQ / Chesterville, ON  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1309 entries, 0 to 1308\n",
      "Data columns (total 14 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   pclass     1309 non-null   int64  \n",
      " 1   survived   1309 non-null   int64  \n",
      " 2   name       1309 non-null   object \n",
      " 3   sex        1309 non-null   object \n",
      " 4   age        1046 non-null   float64\n",
      " 5   sibsp      1309 non-null   int64  \n",
      " 6   parch      1309 non-null   int64  \n",
      " 7   ticket     1309 non-null   object \n",
      " 8   fare       1308 non-null   float64\n",
      " 9   cabin      295 non-null    object \n",
      " 10  embarked   1307 non-null   object \n",
      " 11  boat       486 non-null    object \n",
      " 12  body       121 non-null    float64\n",
      " 13  home.dest  745 non-null    object \n",
      "dtypes: float64(3), int64(4), object(7)\n",
      "memory usage: 143.3+ KB\n",
      "None\n",
      "            pclass     survived          age        sibsp        parch  \\\n",
      "count  1309.000000  1309.000000  1046.000000  1309.000000  1309.000000   \n",
      "mean      2.294882     0.381971    29.881135     0.498854     0.385027   \n",
      "std       0.837836     0.486055    14.413500     1.041658     0.865560   \n",
      "min       1.000000     0.000000     0.166700     0.000000     0.000000   \n",
      "25%       2.000000     0.000000    21.000000     0.000000     0.000000   \n",
      "50%       3.000000     0.000000    28.000000     0.000000     0.000000   \n",
      "75%       3.000000     1.000000    39.000000     1.000000     0.000000   \n",
      "max       3.000000     1.000000    80.000000     8.000000     9.000000   \n",
      "\n",
      "              fare        body  \n",
      "count  1308.000000  121.000000  \n",
      "mean     33.295479  160.809917  \n",
      "std      51.758668   97.696922  \n",
      "min       0.000000    1.000000  \n",
      "25%       7.895800   72.000000  \n",
      "50%      14.454200  155.000000  \n",
      "75%      31.275000  256.000000  \n",
      "max     512.329200  328.000000  \n"
     ]
    }
   ],
   "source": [
    "# Load the Titanic dataset\n",
    "df = pd.read_excel('titanic3.xls')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Get summary info of the dataset \n",
    "print(df.info())\n",
    "\n",
    "# Get descriptive statistics for numerical valeus\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Managing Missing Values\n",
    "\n",
    "Lecture Material: Lecture 3, slides 22–24.\n",
    "\n",
    "- Identify the columns containing missing values.\n",
    "- Develop a strategy to address them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age           263\n",
      "fare            1\n",
      "cabin        1014\n",
      "embarked        2\n",
      "boat          823\n",
      "body         1188\n",
      "home.dest     564\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in each column\n",
    "missing_data = df.isnull().sum()\n",
    "\n",
    "# Display the columns that have missing values and how many\n",
    "missing_data = missing_data[missing_data > 0]\n",
    "print(missing_data)\n",
    "\n",
    "## Filling missing values\n",
    "## I decided to simply fill the missing values with the mode and means rather than erase them\n",
    "# Imputing numerical columns with median values\n",
    "df['age'] = df['age'].fillna(df['age'].median())\n",
    "df['fare'] = df['fare'].fillna(df['fare'].median())\n",
    "df['body'] = df['body'].fillna(df['body'].median())\n",
    "\n",
    "# Imputing categorical columns with the mode value\n",
    "df['embarked'] = df['embarked'].fillna(df['embarked'].mode()[0])\n",
    "df['cabin'] = df['cabin'].fillna(df['cabin'].mode()[0])\n",
    "df['boat'] = df['boat'].fillna(df['boat'].mode()[0])\n",
    "df['home.dest'] = df['home.dest'].fillna(df['home.dest'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Encoding Categorical Variables\n",
    "\n",
    "Lecture material: Lecture 4, slides 10–15, 21.\n",
    "\n",
    "- Identify the categorical variables in the dataset.\n",
    "- Utilize OneHotEncoder to encode them.\n",
    "- Observe the transformation and discuss its impact on machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sex_female sex_male embarked_C embarked_Q embarked_S survived_0 survived_1  \\\n",
      "0        1.0      0.0        0.0        0.0        1.0        0.0        1.0   \n",
      "1        0.0      1.0        0.0        0.0        1.0        0.0        1.0   \n",
      "2        1.0      0.0        0.0        0.0        1.0        1.0        0.0   \n",
      "3        0.0      1.0        0.0        0.0        1.0        1.0        0.0   \n",
      "4        1.0      0.0        0.0        0.0        1.0        1.0        0.0   \n",
      "\n",
      "  pclass_1 pclass_2 pclass_3                                              age  \\\n",
      "0      1.0      0.0      0.0                    Allen, Miss. Elisabeth Walton   \n",
      "1      1.0      0.0      0.0                   Allison, Master. Hudson Trevor   \n",
      "2      1.0      0.0      0.0                     Allison, Miss. Helen Loraine   \n",
      "3      1.0      0.0      0.0             Allison, Mr. Hudson Joshua Creighton   \n",
      "4      1.0      0.0      0.0  Allison, Mrs. Hudson J C (Bessie Waldo Daniels)   \n",
      "\n",
      "    sibsp parch ticket    fare     cabin embarked boat   body  \\\n",
      "0    29.0     0      0   24160  211.3375       B5    2  155.0   \n",
      "1  0.9167     1      2  113781    151.55  C22 C26   11  155.0   \n",
      "2     2.0     1      2  113781    151.55  C22 C26   13  155.0   \n",
      "3    30.0     1      2  113781    151.55  C22 C26   13  135.0   \n",
      "4    25.0     1      2  113781    151.55  C22 C26   13  155.0   \n",
      "\n",
      "                         home.dest  \n",
      "0                     St Louis, MO  \n",
      "1  Montreal, PQ / Chesterville, ON  \n",
      "2  Montreal, PQ / Chesterville, ON  \n",
      "3  Montreal, PQ / Chesterville, ON  \n",
      "4  Montreal, PQ / Chesterville, ON  \n"
     ]
    }
   ],
   "source": [
    "# Define the categorical columns for encoding\n",
    "categorical_columns = ['sex',  'embarked', 'survived', 'pclass']\n",
    "\n",
    "# Create a ColumnTransformer that applies OneHotEncoder to the categorical columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), categorical_columns)\n",
    "    ],\n",
    "    remainder='passthrough'  \n",
    ")\n",
    "\n",
    "# Apply the encoding to the dataset using a pipeline and convert them to a DataFrame\n",
    "df_encoded = preprocessor.fit_transform(df)\n",
    "encoded_columns = preprocessor.transformers_[0][1].get_feature_names_out(categorical_columns)\n",
    "df_encoded = pd.DataFrame(df_encoded, columns=np.append(encoded_columns, df.columns[len(categorical_columns):]))\n",
    "\n",
    "# Check the result\n",
    "print(df_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Feature Scaling\n",
    "\n",
    "Lecture material: Lecture 5, slides 14–20.\n",
    "\n",
    "- Standardize the numerical variables using StandardScaler.\n",
    "- Normalize the numerical variables using MinMaxScaler.\n",
    "- Discuss the differences between standardization and normalization, along with their importance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        age      fare     sibsp     parch      body\n",
      "0 -0.039005  3.442584 -0.479087 -0.445000 -0.018126\n",
      "1 -2.215952  2.286639  0.481288  1.866526 -0.018126\n",
      "2 -2.131977  2.286639  0.481288  1.866526 -0.018126\n",
      "3  0.038512  2.286639  0.481288  1.866526 -0.693162\n",
      "4 -0.349075  2.286639  0.481288  1.866526 -0.018126\n",
      "        age      fare  sibsp     parch      body\n",
      "0  0.361169  0.412503  0.000  0.000000  0.470948\n",
      "1  0.009395  0.295806  0.125  0.222222  0.470948\n",
      "2  0.022964  0.295806  0.125  0.222222  0.470948\n",
      "3  0.373695  0.295806  0.125  0.222222  0.409786\n",
      "4  0.311064  0.295806  0.125  0.222222  0.470948\n"
     ]
    }
   ],
   "source": [
    "# Define the numerical columns to standardize and Fit and transform the numerical columns to standardize them\n",
    "numerical_columns = ['age', 'fare', 'sibsp', 'parch', 'body']\n",
    "scaler = StandardScaler()\n",
    "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "# Check the result\n",
    "print(df[numerical_columns].head())\n",
    "\n",
    "# Create the MinMaxScaler instance and Fit and transform the numerical columns to normalize them\n",
    "min_max_scaler = MinMaxScaler()\n",
    "df[numerical_columns] = min_max_scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "# Check the result\n",
    "print(df[numerical_columns].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Data Splitting\n",
    "\n",
    "Lecture material: Lecture 2, slides 4–7.\n",
    "\n",
    "- Split the dataset into training, validation, and test sets.\n",
    "- Ensure that the split reflects the original distribution of the target variable using stratification.\n",
    "\n",
    "**Note**: a good strategy is to first split the dataset into ‘training’ and ‘others’, and then split ‘others’ into equally sized ‘validation’ and ‘test’ sets. When splitting sets, consider the argument stratify of the train test split\n",
    "method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set target distribution:\n",
      "survived\n",
      "0    0.617834\n",
      "1    0.382166\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Validation set target distribution:\n",
      "survived\n",
      "0    0.618321\n",
      "1    0.381679\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test set target distribution:\n",
      "survived\n",
      "0    0.618321\n",
      "1    0.381679\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assuming 'df' is the DataFrame and 'survival' is the target variable\n",
    "X = df.drop(columns=['survived'])  # Features (excluding target variable)\n",
    "y = df['survived']  # Target variable (survival)\n",
    "\n",
    "# Split into training and 'others' (validation and test)\n",
    "X_train, X_others, y_train, y_others = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n",
    "\n",
    "# Split 'others' into validation and test sets, maintaining target distribution\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_others, y_others, test_size=0.5, stratify=y_others, random_state=42)\n",
    "\n",
    "# Check the distribution of the target variable in each split\n",
    "print(\"Training set target distribution:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nValidation set target distribution:\")\n",
    "print(y_val.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nTest set target distribution:\")\n",
    "print(y_test.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Addressing Class Imbalance\n",
    "\n",
    "Lecture material: Lecture 3, slides 25–27; Lecture 4, slides 4–5.\n",
    "\n",
    "- Apply a method to address class imbalance (e.g., Oversampling Technique (SMOTE), Adaptive Synthetic\n",
    "Sampling Method (ADASYN)).\n",
    "\n",
    "**Note**: You can load a SMOTE and/or ADASYN implementation from the Python module imblearn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution in the training set:\n",
      "pclass\n",
      "3    0.542502\n",
      "1    0.245463\n",
      "2    0.212034\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class distribution after applying SMOTE:\n",
      "pclass\n",
      "3    0.333333\n",
      "2    0.333333\n",
      "1    0.333333\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "## Prepare the dataset\n",
    "# Encode categorical variables (if any) and handle missing values\n",
    "df_encoded = pd.get_dummies(df, drop_first=True)\n",
    "X = df_encoded.drop(columns=['pclass'])\n",
    "y = df_encoded['pclass']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling the features \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Check the class distribution before applying SMOTE\n",
    "print(\"Original class distribution in the training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "# Impute missing values using the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train_scaled)\n",
    "X_test_imputed = imputer.transform(X_test_scaled)\n",
    "\n",
    "## Apply SMOTE to balance the classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_imputed, y_train)\n",
    "\n",
    "# Check class distribution after SMOTE\n",
    "print(\"\\nClass distribution after applying SMOTE:\")\n",
    "print(pd.Series(y_train_resampled).value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Feature Selection\n",
    "\n",
    "Lecture material: Lecture 5, slides 10–14, 19.\n",
    "\n",
    "- Eliminate low variance and highly correlated features.\n",
    "- Why do we carry out tasks 6 and 7 after splitting the dataset into training, validation, and test sets? Could\n",
    "we have conducted them on the entire dataset instead? Please elaborate on your answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining features after variance thresholding: Index(['survived', 'age', 'sibsp', 'fare'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## Drop non-numeric columns from X_train \n",
    "X_train_numeric = X_train.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Step 2: Remove Low Variance Features\n",
    "variance_threshold = VarianceThreshold(threshold=0.01)  # (Set a threshold to remove low variance features)\n",
    "X_train_no_low_variance = variance_threshold.fit_transform(X_train_numeric)\n",
    "\n",
    "# Step 3: Get the feature names of the remaining features after variance reduction\n",
    "remaining_features = X_train_numeric.columns[variance_threshold.get_support()]\n",
    "print(f\"Remaining features after variance thresholding: {remaining_features}\")\n",
    "\n",
    "# Create a new DataFrame with only the selected features\n",
    "X_train_no_low_variance_df = pd.DataFrame(X_train_no_low_variance, columns=remaining_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: Training a Logistic Regression Model\n",
    "\n",
    "Lecture material: Lecture 6, slides 5–9.\n",
    "\n",
    "- Train a Logistic Regression Model to predict whether a passenger survives.\n",
    "\n",
    "**Note**: Use the method predict from the class LogisticRegression with the validation set. Have fun finding\n",
    "a visually appealing way to display the results of the predictions on the validation set. An analysis of model\n",
    "performance is not required and will not affect your final grade for the assignment. However, I won’t stop you from\n",
    "including it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.72%\n",
      "Confusion Matrix:\n",
      "[[125  19]\n",
      " [ 42  76]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.87      0.80       144\n",
      "           1       0.80      0.64      0.71       118\n",
      "\n",
      "    accuracy                           0.77       262\n",
      "   macro avg       0.77      0.76      0.76       262\n",
      "weighted avg       0.77      0.77      0.76       262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1.Loading the Titanic3.xls dataset\n",
    "df = pd.read_excel('titanic3.xls')\n",
    "\n",
    "# 2.Handling missing values \n",
    "df['age'] = df['age'].fillna(df['age'].median())\n",
    "df['fare'] = df['fare'].fillna(df['fare'].median())\n",
    "df['embarked'] = df['embarked'].fillna(df['embarked'].mode()[0])\n",
    "df['cabin'] = df['cabin'].fillna(df['cabin'].mode()[0])\n",
    "df['boat'] = df['boat'].fillna(df['boat'].mode()[0])\n",
    "df = df.dropna(subset=['survived', 'sex'])\n",
    "\n",
    "# 3.Encoding categorical features using one-hot encoding\n",
    "df = pd.get_dummies(df, columns=['sex', 'embarked', 'cabin'], drop_first=True)\n",
    "\n",
    "# 4.Selecting relevant features (excluding 'name', 'ticket' that dont affect the survival)\n",
    "X = df[['age', 'fare', 'sibsp', 'parch', 'sex_male', 'embarked_Q', 'embarked_S']]\n",
    "y = df['survived']\n",
    "\n",
    "# 5.Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 6.Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Training the Logistic Regression model\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = logreg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Displaying results\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
